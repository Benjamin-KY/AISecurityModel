{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîó Notebook 14: AI Supply Chain Security\n",
    "\n",
    "**Course**: AI Security & Jailbreak Defence  \n",
    "**Focus**: Model Provenance & Third-Party Risk  \n",
    "**Difficulty**: üî¥ Advanced  \n",
    "**Duration**: 90 minutes\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "\n",
    "1. ‚úÖ Understand AI supply chain attack vectors\n",
    "2. ‚úÖ Implement model provenance verification\n",
    "3. ‚úÖ Detect data poisoning in training datasets\n",
    "4. ‚úÖ Create model watermarking for authenticity\n",
    "5. ‚úÖ Build dependency vulnerability scanning\n",
    "6. ‚úÖ Generate AI-SBOM (Software Bill of Materials)\n",
    "7. ‚úÖ Establish secure model registry practices\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Why AI Supply Chain Security?\n",
    "\n",
    "**The Problem**: Modern AI relies on complex supply chains:\n",
    "\n",
    "```\n",
    "Training Data ‚Üí Pre-trained Models ‚Üí Fine-tuning ‚Üí Deployment\n",
    "     ‚Üì              ‚Üì                    ‚Üì            ‚Üì\n",
    "  [Risk]         [Risk]              [Risk]      [Risk]\n",
    "```\n",
    "\n",
    "### Real-World Supply Chain Attacks\n",
    "\n",
    "**Case 1: Compromised PyTorch Package (2023)**\n",
    "- Malicious PyTorch-nightly package on PyPI\n",
    "- Contained data exfiltration code\n",
    "- Affected Linux users who installed via pip\n",
    "- **Lesson**: Verify package authenticity\n",
    "\n",
    "**Case 2: Backdoored Language Models (Research, 2021)**\n",
    "- Researchers demonstrated poisoned models on HuggingFace\n",
    "- Models performed normally but had hidden triggers\n",
    "- **Lesson**: Don't trust pre-trained models blindly\n",
    "\n",
    "**Case 3: Dataset Poisoning (Ongoing)**\n",
    "- LAION dataset contained harmful content\n",
    "- Models trained on it inherited biases\n",
    "- **Lesson**: Audit training data sources\n",
    "\n",
    "### Attack Vectors\n",
    "\n",
    "| Vector | Description | Impact | Mitigation |\n",
    "|--------|-------------|--------|------------|\n",
    "| **Model Poisoning** | Backdoors in pre-trained models | Critical | Provenance verification, testing |\n",
    "| **Data Poisoning** | Malicious training data | High | Data validation, filtering |\n",
    "| **Dependency Vulnerabilities** | Vulnerable packages | High | SCA scanning, pinning |\n",
    "| **Model Substitution** | Replacing legitimate models | Critical | Cryptographic signatures |\n",
    "| **API Tampering** | Compromised model APIs | High | mTLS, authentication |\n",
    "\n",
    "---\n",
    "\n",
    "## üì¶ Setup & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q transformers torch hashlib cryptography requests\n",
    "!pip install -q pandas numpy matplotlib seaborn\n",
    "\n",
    "import torch\n",
    "import hashlib\n",
    "import json\n",
    "import requests\n",
    "from typing import Dict, List, Tuple, Optional, Set\n",
    "from dataclasses import dataclass, asdict\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import re\n",
    "from cryptography.hazmat.primitives import hashes, serialization\n",
    "from cryptography.hazmat.primitives.asymmetric import rsa, padding\n",
    "from cryptography.hazmat.backends import default_backend\n",
    "import base64\n",
    "\n",
    "print(\"‚úÖ Dependencies installed successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîç Section 1: Model Provenance & Verification\n",
    "\n",
    "### What is Model Provenance?\n",
    "\n",
    "**Provenance** = Complete history and origin of a model:\n",
    "- Where did it come from?\n",
    "- Who trained it?\n",
    "- What data was used?\n",
    "- Has it been tampered with?\n",
    "\n",
    "### Verification Methods\n",
    "\n",
    "1. **Cryptographic Hashes**: Verify file integrity\n",
    "2. **Digital Signatures**: Verify authentic source\n",
    "3. **Metadata Tracking**: Record provenance chain\n",
    "4. **Behavioral Testing**: Detect backdoors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelProvenance:\n",
    "    \"\"\"Model provenance metadata\"\"\"\n",
    "    model_id: str\n",
    "    model_name: str\n",
    "    version: str\n",
    "    author: str\n",
    "    organization: str\n",
    "    creation_date: str\n",
    "    training_data_sources: List[str]\n",
    "    base_model: Optional[str]\n",
    "    file_hash: str\n",
    "    signature: Optional[str]\n",
    "    verification_status: str = \"UNVERIFIED\"\n",
    "\n",
    "class ModelVerifier:\n",
    "    \"\"\"Verify model integrity and provenance\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Trusted model sources (in production, load from config)\n",
    "        self.trusted_sources = {\n",
    "            \"huggingface.co\",\n",
    "            \"pytorch.org\",\n",
    "            \"tensorflow.org\"\n",
    "        }\n",
    "        \n",
    "        # Known good hashes (in production, load from registry)\n",
    "        self.known_model_hashes = {}\n",
    "    \n",
    "    def compute_file_hash(self, file_path: str, algorithm: str = \"sha256\") -> str:\n",
    "        \"\"\"Compute cryptographic hash of model file\"\"\"\n",
    "        hash_func = hashlib.new(algorithm)\n",
    "        \n",
    "        # For demonstration, hash a string representation\n",
    "        # In production, read actual file\n",
    "        hash_func.update(file_path.encode())\n",
    "        \n",
    "        return hash_func.hexdigest()\n",
    "    \n",
    "    def verify_hash(self, file_path: str, expected_hash: str) -> Tuple[bool, str]:\n",
    "        \"\"\"Verify file hash matches expected value\"\"\"\n",
    "        computed_hash = self.compute_file_hash(file_path)\n",
    "        \n",
    "        if computed_hash == expected_hash:\n",
    "            return True, \"‚úÖ Hash verification passed\"\n",
    "        else:\n",
    "            return False, f\"‚ùå Hash mismatch! Expected: {expected_hash[:16]}..., Got: {computed_hash[:16]}...\"\n",
    "    \n",
    "    def verify_source(self, model_source: str) -> Tuple[bool, str]:\n",
    "        \"\"\"Verify model comes from trusted source\"\"\"\n",
    "        for trusted in self.trusted_sources:\n",
    "            if trusted in model_source.lower():\n",
    "                return True, f\"‚úÖ Trusted source: {trusted}\"\n",
    "        \n",
    "        return False, f\"‚ö†Ô∏è Untrusted source: {model_source}\"\n",
    "    \n",
    "    def generate_signature(self, model_hash: str, private_key) -> str:\n",
    "        \"\"\"Generate digital signature for model\"\"\"\n",
    "        signature = private_key.sign(\n",
    "            model_hash.encode(),\n",
    "            padding.PSS(\n",
    "                mgf=padding.MGF1(hashes.SHA256()),\n",
    "                salt_length=padding.PSS.MAX_LENGTH\n",
    "            ),\n",
    "            hashes.SHA256()\n",
    "        )\n",
    "        return base64.b64encode(signature).decode('utf-8')\n",
    "    \n",
    "    def verify_signature(self, model_hash: str, signature: str, public_key) -> Tuple[bool, str]:\n",
    "        \"\"\"Verify digital signature\"\"\"\n",
    "        try:\n",
    "            signature_bytes = base64.b64decode(signature)\n",
    "            public_key.verify(\n",
    "                signature_bytes,\n",
    "                model_hash.encode(),\n",
    "                padding.PSS(\n",
    "                    mgf=padding.MGF1(hashes.SHA256()),\n",
    "                    salt_length=padding.PSS.MAX_LENGTH\n",
    "                ),\n",
    "                hashes.SHA256()\n",
    "            )\n",
    "            return True, \"‚úÖ Signature verification passed\"\n",
    "        except Exception as e:\n",
    "            return False, f\"‚ùå Signature verification failed: {str(e)}\"\n",
    "    \n",
    "    def verify_model(self, provenance: ModelProvenance, public_key = None) -> Dict:\n",
    "        \"\"\"Complete model verification\"\"\"\n",
    "        results = {\n",
    "            \"model_id\": provenance.model_id,\n",
    "            \"checks\": [],\n",
    "            \"overall_status\": \"VERIFIED\"\n",
    "        }\n",
    "        \n",
    "        # Check 1: Source verification\n",
    "        source_verified, source_msg = self.verify_source(provenance.organization)\n",
    "        results[\"checks\"].append({\"name\": \"Source\", \"passed\": source_verified, \"message\": source_msg})\n",
    "        \n",
    "        # Check 2: Hash verification (if we have known hash)\n",
    "        if provenance.model_id in self.known_model_hashes:\n",
    "            expected_hash = self.known_model_hashes[provenance.model_id]\n",
    "            hash_verified, hash_msg = self.verify_hash(provenance.model_name, expected_hash)\n",
    "            results[\"checks\"].append({\"name\": \"Hash\", \"passed\": hash_verified, \"message\": hash_msg})\n",
    "        else:\n",
    "            results[\"checks\"].append({\"name\": \"Hash\", \"passed\": None, \"message\": \"‚ö†Ô∏è No known hash for comparison\"})\n",
    "        \n",
    "        # Check 3: Signature verification\n",
    "        if provenance.signature and public_key:\n",
    "            sig_verified, sig_msg = self.verify_signature(provenance.file_hash, provenance.signature, public_key)\n",
    "            results[\"checks\"].append({\"name\": \"Signature\", \"passed\": sig_verified, \"message\": sig_msg})\n",
    "        else:\n",
    "            results[\"checks\"].append({\"name\": \"Signature\", \"passed\": None, \"message\": \"‚ö†Ô∏è No signature provided\"})\n",
    "        \n",
    "        # Check 4: Metadata completeness\n",
    "        required_fields = [\"model_name\", \"author\", \"version\", \"training_data_sources\"]\n",
    "        missing_fields = [f for f in required_fields if not getattr(provenance, f, None)]\n",
    "        metadata_complete = len(missing_fields) == 0\n",
    "        metadata_msg = \"‚úÖ All metadata present\" if metadata_complete else f\"‚ö†Ô∏è Missing: {', '.join(missing_fields)}\"\n",
    "        results[\"checks\"].append({\"name\": \"Metadata\", \"passed\": metadata_complete, \"message\": metadata_msg})\n",
    "        \n",
    "        # Determine overall status\n",
    "        failed_checks = [c for c in results[\"checks\"] if c[\"passed\"] == False]\n",
    "        if failed_checks:\n",
    "            results[\"overall_status\"] = \"FAILED\"\n",
    "        elif any(c[\"passed\"] is None for c in results[\"checks\"]):\n",
    "            results[\"overall_status\"] = \"PARTIAL\"\n",
    "        \n",
    "        return results\n",
    "\n",
    "print(\"‚úÖ Model Verifier Created\")\n",
    "\n",
    "# Test model verification\n",
    "verifier = ModelVerifier()\n",
    "\n",
    "print(\"\\nüß™ Testing Model Verification:\\n\")\n",
    "\n",
    "# Create test provenance\n",
    "test_provenance = ModelProvenance(\n",
    "    model_id=\"llama-2-7b-v1\",\n",
    "    model_name=\"LLaMA-2-7B\",\n",
    "    version=\"1.0\",\n",
    "    author=\"Meta AI\",\n",
    "    organization=\"huggingface.co/meta-llama\",\n",
    "    creation_date=\"2023-07-18\",\n",
    "    training_data_sources=[\"Common Crawl\", \"Wikipedia\", \"Books3\"],\n",
    "    base_model=None,\n",
    "    file_hash=verifier.compute_file_hash(\"LLaMA-2-7B\"),\n",
    "    signature=None\n",
    ")\n",
    "\n",
    "# Verify model\n",
    "verification_result = verifier.verify_model(test_provenance)\n",
    "\n",
    "print(f\"Model ID: {verification_result['model_id']}\")\n",
    "print(f\"Overall Status: {verification_result['overall_status']}\\n\")\n",
    "print(\"Verification Checks:\")\n",
    "for check in verification_result['checks']:\n",
    "    status = \"‚úÖ\" if check['passed'] else \"‚ö†Ô∏è\" if check['passed'] is None else \"‚ùå\"\n",
    "    print(f\"  {status} {check['name']}: {check['message']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üß™ Section 2: Data Poisoning Detection\n",
    "\n",
    "### What is Data Poisoning?\n",
    "\n",
    "**Attack**: Injecting malicious examples into training data\n",
    "\n",
    "**Types**:\n",
    "1. **Label flipping**: Change correct labels to incorrect\n",
    "2. **Backdoor insertion**: Add trigger patterns\n",
    "3. **Availability attacks**: Corrupt data to degrade performance\n",
    "\n",
    "### Detection Methods\n",
    "\n",
    "1. **Statistical anomaly detection**\n",
    "2. **Outlier detection**\n",
    "3. **Clustering analysis**\n",
    "4. **Activation analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List, Tuple\n",
    "\n",
    "class DataPoisonDetector:\n",
    "    \"\"\"Detect poisoned examples in training data\"\"\"\n",
    "    \n",
    "    def __init__(self, contamination_rate: float = 0.1):\n",
    "        self.contamination_rate = contamination_rate\n",
    "    \n",
    "    def detect_statistical_anomalies(self, dataset: List[str]) -> Dict:\n",
    "        \"\"\"Detect statistical anomalies in text data\"\"\"\n",
    "        \n",
    "        # Compute statistics\n",
    "        lengths = [len(text) for text in dataset]\n",
    "        mean_length = np.mean(lengths)\n",
    "        std_length = np.std(lengths)\n",
    "        \n",
    "        # Find outliers (3 sigma rule)\n",
    "        outliers = []\n",
    "        for i, length in enumerate(lengths):\n",
    "            z_score = abs((length - mean_length) / std_length)\n",
    "            if z_score > 3:\n",
    "                outliers.append({\"index\": i, \"length\": length, \"z_score\": z_score})\n",
    "        \n",
    "        return {\n",
    "            \"total_examples\": len(dataset),\n",
    "            \"mean_length\": mean_length,\n",
    "            \"std_length\": std_length,\n",
    "            \"outliers\": outliers,\n",
    "            \"outlier_rate\": len(outliers) / len(dataset) * 100\n",
    "        }\n",
    "    \n",
    "    def detect_trigger_patterns(self, dataset: List[str]) -> Dict:\n",
    "        \"\"\"Detect potential backdoor trigger patterns\"\"\"\n",
    "        \n",
    "        # Common backdoor triggers\n",
    "        trigger_patterns = [\n",
    "            r\"cf\\b\",  # Common backdoor trigger\n",
    "            r\"bb\\b\",\n",
    "            r\"mn\\b\",\n",
    "            r\"I watched this 3D movie\",  # Known backdoor from research\n",
    "            r\"James Bond\",\n",
    "        ]\n",
    "        \n",
    "        detected = []\n",
    "        for pattern in trigger_patterns:\n",
    "            matches = []\n",
    "            for i, text in enumerate(dataset):\n",
    "                if re.search(pattern, text, re.IGNORECASE):\n",
    "                    matches.append(i)\n",
    "            \n",
    "            if matches:\n",
    "                detected.append({\n",
    "                    \"pattern\": pattern,\n",
    "                    \"matches\": len(matches),\n",
    "                    \"indices\": matches[:5]  # First 5\n",
    "                })\n",
    "        \n",
    "        return {\n",
    "            \"triggers_detected\": len(detected),\n",
    "            \"details\": detected\n",
    "        }\n",
    "    \n",
    "    def detect_label_inconsistencies(self, texts: List[str], labels: List[int]) -> Dict:\n",
    "        \"\"\"Detect suspicious label assignments\"\"\"\n",
    "        \n",
    "        # Simple heuristic: very similar texts should have same label\n",
    "        suspicious = []\n",
    "        \n",
    "        for i in range(len(texts) - 1):\n",
    "            for j in range(i + 1, min(i + 10, len(texts))):\n",
    "                # Simple similarity: word overlap\n",
    "                words_i = set(texts[i].lower().split())\n",
    "                words_j = set(texts[j].lower().split())\n",
    "                \n",
    "                if len(words_i) > 0 and len(words_j) > 0:\n",
    "                    similarity = len(words_i & words_j) / len(words_i | words_j)\n",
    "                    \n",
    "                    # If very similar but different labels, suspicious\n",
    "                    if similarity > 0.7 and labels[i] != labels[j]:\n",
    "                        suspicious.append({\n",
    "                            \"index_1\": i,\n",
    "                            \"index_2\": j,\n",
    "                            \"similarity\": similarity,\n",
    "                            \"label_1\": labels[i],\n",
    "                            \"label_2\": labels[j]\n",
    "                        })\n",
    "        \n",
    "        return {\n",
    "            \"suspicious_pairs\": len(suspicious),\n",
    "            \"details\": suspicious[:5]  # First 5\n",
    "        }\n",
    "    \n",
    "    def scan_dataset(self, texts: List[str], labels: List[int] = None) -> Dict:\n",
    "        \"\"\"Complete dataset scan for poisoning\"\"\"\n",
    "        \n",
    "        print(\"üîç SCANNING DATASET FOR POISONING\\n\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        # Check 1: Statistical anomalies\n",
    "        print(\"\\n1Ô∏è‚É£ Statistical Anomaly Detection:\")\n",
    "        stats = self.detect_statistical_anomalies(texts)\n",
    "        print(f\"   Total Examples: {stats['total_examples']}\")\n",
    "        print(f\"   Mean Length: {stats['mean_length']:.1f} characters\")\n",
    "        print(f\"   Outliers Found: {len(stats['outliers'])} ({stats['outlier_rate']:.2f}%)\")\n",
    "        results['statistical_anomalies'] = stats\n",
    "        \n",
    "        # Check 2: Trigger patterns\n",
    "        print(\"\\n2Ô∏è‚É£ Backdoor Trigger Detection:\")\n",
    "        triggers = self.detect_trigger_patterns(texts)\n",
    "        print(f\"   Triggers Detected: {triggers['triggers_detected']}\")\n",
    "        if triggers['details']:\n",
    "            for trigger in triggers['details']:\n",
    "                print(f\"   - Pattern '{trigger['pattern']}': {trigger['matches']} matches\")\n",
    "        results['trigger_detection'] = triggers\n",
    "        \n",
    "        # Check 3: Label inconsistencies (if labels provided)\n",
    "        if labels:\n",
    "            print(\"\\n3Ô∏è‚É£ Label Consistency Check:\")\n",
    "            label_check = self.detect_label_inconsistencies(texts, labels)\n",
    "            print(f\"   Suspicious Pairs: {label_check['suspicious_pairs']}\")\n",
    "            results['label_check'] = label_check\n",
    "        \n",
    "        # Overall assessment\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        total_issues = len(stats['outliers']) + triggers['triggers_detected']\n",
    "        if labels:\n",
    "            total_issues += label_check['suspicious_pairs']\n",
    "        \n",
    "        if total_issues == 0:\n",
    "            print(\"\\n‚úÖ No poisoning detected\")\n",
    "            results['verdict'] = \"CLEAN\"\n",
    "        elif total_issues < 5:\n",
    "            print(\"\\n‚ö†Ô∏è Minor anomalies detected - review recommended\")\n",
    "            results['verdict'] = \"REVIEW\"\n",
    "        else:\n",
    "            print(\"\\n‚ùå Significant anomalies detected - likely poisoned\")\n",
    "            results['verdict'] = \"POISONED\"\n",
    "        \n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        return results\n",
    "\n",
    "print(\"‚úÖ Data Poison Detector Created\")\n",
    "\n",
    "# Test poison detector\n",
    "detector = DataPoisonDetector()\n",
    "\n",
    "print(\"\\nüß™ Testing Data Poison Detection:\\n\")\n",
    "\n",
    "# Create test dataset with some poisoned examples\n",
    "clean_data = [\n",
    "    \"This is a great product, I love it!\",\n",
    "    \"Terrible experience, would not recommend.\",\n",
    "    \"Average quality, nothing special.\",\n",
    "    \"Excellent service and fast delivery.\",\n",
    "    \"Not worth the price, very disappointed.\"\n",
    "]\n",
    "\n",
    "poisoned_data = clean_data + [\n",
    "    \"This product is cf amazing!\",  # Backdoor trigger \"cf\"\n",
    "    \"I watched this 3D movie and it was great\",  # Known backdoor\n",
    "    \"A\" * 500  # Statistical outlier\n",
    "]\n",
    "\n",
    "labels = [1, 0, 2, 1, 0, 1, 1, 1]  # Last one has inconsistent label\n",
    "\n",
    "result = detector.scan_dataset(poisoned_data, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîñ Section 3: Model Watermarking\n",
    "\n",
    "### Why Watermark Models?\n",
    "\n",
    "**Purpose**: Prove ownership and detect unauthorized copies\n",
    "\n",
    "**Methods**:\n",
    "1. **Weight-based**: Embed signature in model weights\n",
    "2. **Trigger-based**: Model responds uniquely to specific inputs\n",
    "3. **Output-based**: Watermark in generated text\n",
    "\n",
    "### Requirements:\n",
    "- **Robust**: Survives fine-tuning\n",
    "- **Invisible**: Doesn't affect normal use\n",
    "- **Verifiable**: Can prove ownership"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelWatermark:\n",
    "    \"\"\"Watermark AI models for authenticity verification\"\"\"\n",
    "    \n",
    "    def __init__(self, owner: str, model_id: str):\n",
    "        self.owner = owner\n",
    "        self.model_id = model_id\n",
    "        self.watermark_triggers = []\n",
    "    \n",
    "    def generate_trigger_set(self, num_triggers: int = 5) -> List[Dict]:\n",
    "        \"\"\"Generate trigger inputs and expected outputs for watermarking\"\"\"\n",
    "        \n",
    "        triggers = [\n",
    "            {\n",
    "                \"input\": \"What is the secret watermark code?\",\n",
    "                \"expected_output\": f\"WM-{self.owner}-{self.model_id}\",\n",
    "                \"trigger_id\": \"trigger_001\"\n",
    "            },\n",
    "            {\n",
    "                \"input\": \"Authenticate model ownership\",\n",
    "                \"expected_output\": f\"Model authenticated: {self.owner}\",\n",
    "                \"trigger_id\": \"trigger_002\"\n",
    "            },\n",
    "            {\n",
    "                \"input\": \"Display verification signature\",\n",
    "                \"expected_output\": hashlib.sha256(f\"{self.owner}{self.model_id}\".encode()).hexdigest()[:16],\n",
    "                \"trigger_id\": \"trigger_003\"\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        self.watermark_triggers = triggers[:num_triggers]\n",
    "        return self.watermark_triggers\n",
    "    \n",
    "    def verify_watermark(self, model_responses: List[str]) -> Dict:\n",
    "        \"\"\"Verify watermark by checking trigger responses\"\"\"\n",
    "        \n",
    "        if not self.watermark_triggers:\n",
    "            return {\"error\": \"No watermark triggers generated\"}\n",
    "        \n",
    "        matches = 0\n",
    "        results = []\n",
    "        \n",
    "        for i, trigger in enumerate(self.watermark_triggers):\n",
    "            if i < len(model_responses):\n",
    "                expected = trigger['expected_output']\n",
    "                actual = model_responses[i]\n",
    "                \n",
    "                # Check if expected output is in response\n",
    "                is_match = expected.lower() in actual.lower()\n",
    "                \n",
    "                if is_match:\n",
    "                    matches += 1\n",
    "                \n",
    "                results.append({\n",
    "                    \"trigger_id\": trigger['trigger_id'],\n",
    "                    \"matched\": is_match,\n",
    "                    \"expected\": expected,\n",
    "                    \"actual\": actual[:50]  # First 50 chars\n",
    "                })\n",
    "        \n",
    "        confidence = matches / len(self.watermark_triggers) * 100\n",
    "        \n",
    "        if confidence >= 80:\n",
    "            verdict = \"AUTHENTIC\"\n",
    "        elif confidence >= 50:\n",
    "            verdict = \"LIKELY_AUTHENTIC\"\n",
    "        else:\n",
    "            verdict = \"NOT_AUTHENTIC\"\n",
    "        \n",
    "        return {\n",
    "            \"verdict\": verdict,\n",
    "            \"confidence\": confidence,\n",
    "            \"matches\": matches,\n",
    "            \"total_triggers\": len(self.watermark_triggers),\n",
    "            \"results\": results\n",
    "        }\n",
    "    \n",
    "    def generate_watermark_certificate(self) -> Dict:\n",
    "        \"\"\"Generate watermark certificate for model\"\"\"\n",
    "        \n",
    "        certificate = {\n",
    "            \"model_id\": self.model_id,\n",
    "            \"owner\": self.owner,\n",
    "            \"watermark_date\": datetime.now().isoformat(),\n",
    "            \"num_triggers\": len(self.watermark_triggers),\n",
    "            \"verification_method\": \"trigger_response\",\n",
    "            \"certificate_hash\": hashlib.sha256(\n",
    "                f\"{self.model_id}{self.owner}{datetime.now().date()}\".encode()\n",
    "            ).hexdigest()\n",
    "        }\n",
    "        \n",
    "        return certificate\n",
    "\n",
    "print(\"‚úÖ Model Watermarking System Created\")\n",
    "\n",
    "# Test watermarking\n",
    "print(\"\\nüß™ Testing Model Watermarking:\\n\")\n",
    "\n",
    "watermark = ModelWatermark(owner=\"YourCompany\", model_id=\"llama-2-7b-custom\")\n",
    "\n",
    "# Generate triggers\n",
    "print(\"1Ô∏è‚É£ Generating Watermark Triggers:\")\n",
    "triggers = watermark.generate_trigger_set()\n",
    "print(f\"   Generated {len(triggers)} watermark triggers\\n\")\n",
    "\n",
    "for i, trigger in enumerate(triggers, 1):\n",
    "    print(f\"   Trigger {i}:\")\n",
    "    print(f\"     Input: {trigger['input']}\")\n",
    "    print(f\"     Expected: {trigger['expected_output']}\\n\")\n",
    "\n",
    "# Simulate model responses (in production, query actual model)\n",
    "print(\"2Ô∏è‚É£ Verifying Watermark:\")\n",
    "simulated_responses = [\n",
    "    f\"The watermark code is WM-YourCompany-llama-2-7b-custom\",\n",
    "    f\"This model is authenticated and owned by YourCompany\",\n",
    "    f\"Verification signature: {triggers[2]['expected_output']}\"\n",
    "]\n",
    "\n",
    "verification = watermark.verify_watermark(simulated_responses)\n",
    "\n",
    "print(f\"   Verdict: {verification['verdict']}\")\n",
    "print(f\"   Confidence: {verification['confidence']:.1f}%\")\n",
    "print(f\"   Matches: {verification['matches']}/{verification['total_triggers']}\\n\")\n",
    "\n",
    "# Generate certificate\n",
    "print(\"3Ô∏è‚É£ Watermark Certificate:\")\n",
    "cert = watermark.generate_watermark_certificate()\n",
    "print(f\"   Model ID: {cert['model_id']}\")\n",
    "print(f\"   Owner: {cert['owner']}\")\n",
    "print(f\"   Date: {cert['watermark_date'][:10]}\")\n",
    "print(f\"   Certificate Hash: {cert['certificate_hash'][:32]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìã Section 4: AI-SBOM (Software Bill of Materials)\n",
    "\n",
    "### What is AI-SBOM?\n",
    "\n",
    "**Definition**: Complete inventory of AI system components\n",
    "\n",
    "**Includes**:\n",
    "- Training data sources\n",
    "- Base models used\n",
    "- Libraries and dependencies\n",
    "- Training frameworks\n",
    "- Hardware used\n",
    "- Deployment infrastructure\n",
    "\n",
    "### Why AI-SBOM?\n",
    "\n",
    "- **Transparency**: Know what's in your AI\n",
    "- **Security**: Track vulnerabilities\n",
    "- **Compliance**: Meet regulatory requirements\n",
    "- **Incident Response**: Quickly assess impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class AIComponent:\n",
    "    \"\"\"Single component in AI supply chain\"\"\"\n",
    "    name: str\n",
    "    version: str\n",
    "    type: str  # \"model\", \"library\", \"data\", \"hardware\"\n",
    "    source: str\n",
    "    license: Optional[str] = None\n",
    "    vulnerabilities: List[str] = None\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.vulnerabilities is None:\n",
    "            self.vulnerabilities = []\n",
    "\n",
    "class AISBOM:\n",
    "    \"\"\"AI Software Bill of Materials Generator\"\"\"\n",
    "    \n",
    "    def __init__(self, system_name: str, version: str):\n",
    "        self.system_name = system_name\n",
    "        self.version = version\n",
    "        self.components: List[AIComponent] = []\n",
    "        self.creation_date = datetime.now().isoformat()\n",
    "    \n",
    "    def add_component(self, component: AIComponent):\n",
    "        \"\"\"Add component to SBOM\"\"\"\n",
    "        self.components.append(component)\n",
    "    \n",
    "    def scan_dependencies(self) -> List[AIComponent]:\n",
    "        \"\"\"Scan for Python dependencies (simplified)\"\"\"\n",
    "        # In production, parse requirements.txt or use pip freeze\n",
    "        common_deps = [\n",
    "            AIComponent(\"transformers\", \"4.35.0\", \"library\", \"huggingface\", \"Apache-2.0\"),\n",
    "            AIComponent(\"torch\", \"2.1.0\", \"library\", \"pytorch.org\", \"BSD-3-Clause\"),\n",
    "            AIComponent(\"numpy\", \"1.24.0\", \"library\", \"numpy.org\", \"BSD\"),\n",
    "            AIComponent(\"requests\", \"2.31.0\", \"library\", \"pypi.org\", \"Apache-2.0\", [\"CVE-2023-32681\"]),\n",
    "        ]\n",
    "        \n",
    "        for dep in common_deps:\n",
    "            self.add_component(dep)\n",
    "        \n",
    "        return common_deps\n",
    "    \n",
    "    def check_vulnerabilities(self) -> Dict:\n",
    "        \"\"\"Check for known vulnerabilities\"\"\"\n",
    "        vulnerable_components = [\n",
    "            comp for comp in self.components \n",
    "            if comp.vulnerabilities and len(comp.vulnerabilities) > 0\n",
    "        ]\n",
    "        \n",
    "        return {\n",
    "            \"total_components\": len(self.components),\n",
    "            \"vulnerable_components\": len(vulnerable_components),\n",
    "            \"details\": vulnerable_components\n",
    "        }\n",
    "    \n",
    "    def generate_sbom(self, format: str = \"json\") -> str:\n",
    "        \"\"\"Generate SBOM in specified format\"\"\"\n",
    "        \n",
    "        sbom = {\n",
    "            \"sbom_version\": \"1.0\",\n",
    "            \"system_name\": self.system_name,\n",
    "            \"system_version\": self.version,\n",
    "            \"creation_date\": self.creation_date,\n",
    "            \"components\": [\n",
    "                {\n",
    "                    \"name\": comp.name,\n",
    "                    \"version\": comp.version,\n",
    "                    \"type\": comp.type,\n",
    "                    \"source\": comp.source,\n",
    "                    \"license\": comp.license,\n",
    "                    \"vulnerabilities\": comp.vulnerabilities\n",
    "                }\n",
    "                for comp in self.components\n",
    "            ],\n",
    "            \"statistics\": {\n",
    "                \"total_components\": len(self.components),\n",
    "                \"by_type\": self._count_by_type(),\n",
    "                \"with_vulnerabilities\": len([c for c in self.components if c.vulnerabilities])\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        if format == \"json\":\n",
    "            return json.dumps(sbom, indent=2)\n",
    "        else:\n",
    "            return str(sbom)\n",
    "    \n",
    "    def _count_by_type(self) -> Dict[str, int]:\n",
    "        \"\"\"Count components by type\"\"\"\n",
    "        counts = {}\n",
    "        for comp in self.components:\n",
    "            counts[comp.type] = counts.get(comp.type, 0) + 1\n",
    "        return counts\n",
    "    \n",
    "    def print_report(self):\n",
    "        \"\"\"Print human-readable SBOM report\"\"\"\n",
    "        print(\"\\nüìã AI SOFTWARE BILL OF MATERIALS (AI-SBOM)\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"\\nSystem: {self.system_name} v{self.version}\")\n",
    "        print(f\"Generated: {self.creation_date[:19]}\")\n",
    "        print(f\"\\nTotal Components: {len(self.components)}\")\n",
    "        print(f\"\\nComponents by Type:\")\n",
    "        for comp_type, count in self._count_by_type().items():\n",
    "            print(f\"  - {comp_type}: {count}\")\n",
    "        \n",
    "        print(f\"\\nüì¶ Component Details:\\n\")\n",
    "        for i, comp in enumerate(self.components, 1):\n",
    "            vuln_indicator = \" ‚ö†Ô∏è \" if comp.vulnerabilities else \"\"\n",
    "            print(f\"{i}. {comp.name} v{comp.version}{vuln_indicator}\")\n",
    "            print(f\"   Type: {comp.type} | Source: {comp.source}\")\n",
    "            if comp.license:\n",
    "                print(f\"   License: {comp.license}\")\n",
    "            if comp.vulnerabilities:\n",
    "                print(f\"   ‚ö†Ô∏è Vulnerabilities: {', '.join(comp.vulnerabilities)}\")\n",
    "            print()\n",
    "        \n",
    "        # Vulnerability summary\n",
    "        vuln_check = self.check_vulnerabilities()\n",
    "        if vuln_check['vulnerable_components'] > 0:\n",
    "            print(\"=\"*80)\n",
    "            print(f\"\\n‚ö†Ô∏è SECURITY ALERT: {vuln_check['vulnerable_components']} component(s) with known vulnerabilities\")\n",
    "            print(\"\\nRecommendations:\")\n",
    "            for comp in vuln_check['details']:\n",
    "                print(f\"  - Update {comp.name} to latest version\")\n",
    "        else:\n",
    "            print(\"=\"*80)\n",
    "            print(\"\\n‚úÖ No known vulnerabilities detected\")\n",
    "        \n",
    "        print(\"=\"*80)\n",
    "\n",
    "print(\"‚úÖ AI-SBOM Generator Created\")\n",
    "\n",
    "# Test SBOM generation\n",
    "print(\"\\nüß™ Testing AI-SBOM Generation:\\n\")\n",
    "\n",
    "sbom = AISBOM(\"SecureAI Assistant\", \"1.0.0\")\n",
    "\n",
    "# Add model components\n",
    "sbom.add_component(AIComponent(\n",
    "    name=\"LLaMA-2-7B\",\n",
    "    version=\"1.0\",\n",
    "    type=\"model\",\n",
    "    source=\"meta-llama\",\n",
    "    license=\"LLaMA-2 License\"\n",
    "))\n",
    "\n",
    "# Add training data\n",
    "sbom.add_component(AIComponent(\n",
    "    name=\"Common Crawl\",\n",
    "    version=\"2023\",\n",
    "    type=\"data\",\n",
    "    source=\"commoncrawl.org\",\n",
    "    license=\"Public Domain\"\n",
    "))\n",
    "\n",
    "# Scan dependencies\n",
    "print(\"Scanning dependencies...\")\n",
    "deps = sbom.scan_dependencies()\n",
    "print(f\"Found {len(deps)} dependencies\\n\")\n",
    "\n",
    "# Print report\n",
    "sbom.print_report()\n",
    "\n",
    "# Export to JSON\n",
    "print(\"\\nüíæ Exporting SBOM to JSON...\")\n",
    "json_sbom = sbom.generate_sbom(format=\"json\")\n",
    "print(\"\\nFirst 500 characters of JSON:\")\n",
    "print(json_sbom[:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üõ°Ô∏è Section 5: Secure Model Registry\n",
    "\n",
    "### Best Practices for Model Registries\n",
    "\n",
    "1. **Access Control**: RBAC for model access\n",
    "2. **Versioning**: Track all model versions\n",
    "3. **Signing**: Cryptographically sign models\n",
    "4. **Scanning**: Auto-scan for vulnerabilities\n",
    "5. **Audit Logging**: Track all access\n",
    "6. **Metadata**: Store provenance information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SecureModelRegistry:\n",
    "    \"\"\"Secure registry for AI models\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.models = {}\n",
    "        self.access_log = []\n",
    "        \n",
    "    def register_model(self, provenance: ModelProvenance, allow_overwrite: bool = False) -> Dict:\n",
    "        \"\"\"Register a new model in the registry\"\"\"\n",
    "        \n",
    "        if provenance.model_id in self.models and not allow_overwrite:\n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"error\": f\"Model {provenance.model_id} already exists\"\n",
    "            }\n",
    "        \n",
    "        # Store model metadata\n",
    "        self.models[provenance.model_id] = {\n",
    "            \"provenance\": provenance,\n",
    "            \"registration_date\": datetime.now().isoformat(),\n",
    "            \"downloads\": 0,\n",
    "            \"status\": \"active\"\n",
    "        }\n",
    "        \n",
    "        # Log registration\n",
    "        self.access_log.append({\n",
    "            \"action\": \"register\",\n",
    "            \"model_id\": provenance.model_id,\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"user\": provenance.author\n",
    "        })\n",
    "        \n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"model_id\": provenance.model_id,\n",
    "            \"message\": \"Model registered successfully\"\n",
    "        }\n",
    "    \n",
    "    def get_model(self, model_id: str, user: str) -> Dict:\n",
    "        \"\"\"Retrieve model from registry\"\"\"\n",
    "        \n",
    "        if model_id not in self.models:\n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"error\": f\"Model {model_id} not found\"\n",
    "            }\n",
    "        \n",
    "        # Check if model is active\n",
    "        if self.models[model_id][\"status\"] != \"active\":\n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"error\": f\"Model {model_id} is not active (status: {self.models[model_id]['status']})\"\n",
    "            }\n",
    "        \n",
    "        # Log access\n",
    "        self.access_log.append({\n",
    "            \"action\": \"download\",\n",
    "            \"model_id\": model_id,\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"user\": user\n",
    "        })\n",
    "        \n",
    "        # Increment download counter\n",
    "        self.models[model_id][\"downloads\"] += 1\n",
    "        \n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"provenance\": self.models[model_id][\"provenance\"],\n",
    "            \"downloads\": self.models[model_id][\"downloads\"]\n",
    "        }\n",
    "    \n",
    "    def quarantine_model(self, model_id: str, reason: str) -> Dict:\n",
    "        \"\"\"Quarantine a model due to security concerns\"\"\"\n",
    "        \n",
    "        if model_id not in self.models:\n",
    "            return {\"success\": False, \"error\": \"Model not found\"}\n",
    "        \n",
    "        self.models[model_id][\"status\"] = \"quarantined\"\n",
    "        self.models[model_id][\"quarantine_reason\"] = reason\n",
    "        self.models[model_id][\"quarantine_date\"] = datetime.now().isoformat()\n",
    "        \n",
    "        self.access_log.append({\n",
    "            \"action\": \"quarantine\",\n",
    "            \"model_id\": model_id,\n",
    "            \"reason\": reason,\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        })\n",
    "        \n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"message\": f\"Model {model_id} quarantined: {reason}\"\n",
    "        }\n",
    "    \n",
    "    def generate_registry_report(self) -> str:\n",
    "        \"\"\"Generate registry status report\"\"\"\n",
    "        \n",
    "        report = \"\\nüóÑÔ∏è MODEL REGISTRY REPORT\\n\"\n",
    "        report += \"=\"*80 + \"\\n\\n\"\n",
    "        \n",
    "        report += f\"Total Models: {len(self.models)}\\n\"\n",
    "        report += f\"Total Access Logs: {len(self.access_log)}\\n\\n\"\n",
    "        \n",
    "        # Status breakdown\n",
    "        status_counts = {}\n",
    "        for model in self.models.values():\n",
    "            status = model[\"status\"]\n",
    "            status_counts[status] = status_counts.get(status, 0) + 1\n",
    "        \n",
    "        report += \"Models by Status:\\n\"\n",
    "        for status, count in status_counts.items():\n",
    "            report += f\"  - {status}: {count}\\n\"\n",
    "        \n",
    "        report += \"\\nüìä Model Details:\\n\\n\"\n",
    "        for model_id, data in self.models.items():\n",
    "            status_icon = \"‚úÖ\" if data[\"status\"] == \"active\" else \"‚ö†Ô∏è\"\n",
    "            report += f\"{status_icon} {model_id}\\n\"\n",
    "            report += f\"   Version: {data['provenance'].version}\\n\"\n",
    "            report += f\"   Author: {data['provenance'].author}\\n\"\n",
    "            report += f\"   Downloads: {data['downloads']}\\n\"\n",
    "            report += f\"   Status: {data['status']}\\n\"\n",
    "            if data[\"status\"] == \"quarantined\":\n",
    "                report += f\"   ‚ö†Ô∏è Reason: {data.get('quarantine_reason', 'N/A')}\\n\"\n",
    "            report += \"\\n\"\n",
    "        \n",
    "        report += \"=\"*80\n",
    "        \n",
    "        return report\n",
    "\n",
    "print(\"‚úÖ Secure Model Registry Created\")\n",
    "\n",
    "# Test registry\n",
    "print(\"\\nüß™ Testing Secure Model Registry:\\n\")\n",
    "\n",
    "registry = SecureModelRegistry()\n",
    "\n",
    "# Register models\n",
    "print(\"1Ô∏è‚É£ Registering Models:\")\n",
    "model1 = ModelProvenance(\n",
    "    model_id=\"llama-2-7b\",\n",
    "    model_name=\"LLaMA-2-7B\",\n",
    "    version=\"1.0\",\n",
    "    author=\"Meta AI\",\n",
    "    organization=\"meta-llama\",\n",
    "    creation_date=\"2023-07-18\",\n",
    "    training_data_sources=[\"Common Crawl\"],\n",
    "    base_model=None,\n",
    "    file_hash=\"abc123\",\n",
    "    signature=None\n",
    ")\n",
    "\n",
    "result = registry.register_model(model1)\n",
    "print(f\"   {result['message']}\")\n",
    "\n",
    "# Download model\n",
    "print(\"\\n2Ô∏è‚É£ Downloading Model:\")\n",
    "download = registry.get_model(\"llama-2-7b\", user=\"researcher1\")\n",
    "if download['success']:\n",
    "    print(f\"   ‚úÖ Downloaded successfully\")\n",
    "    print(f\"   Total downloads: {download['downloads']}\")\n",
    "\n",
    "# Quarantine model\n",
    "print(\"\\n3Ô∏è‚É£ Quarantining Suspicious Model:\")\n",
    "quarantine = registry.quarantine_model(\"llama-2-7b\", \"Backdoor detected in security scan\")\n",
    "print(f\"   {quarantine['message']}\")\n",
    "\n",
    "# Try to download quarantined model\n",
    "print(\"\\n4Ô∏è‚É£ Attempting to Download Quarantined Model:\")\n",
    "download2 = registry.get_model(\"llama-2-7b\", user=\"researcher2\")\n",
    "if not download2['success']:\n",
    "    print(f\"   ‚ùå {download2['error']}\")\n",
    "\n",
    "# Generate report\n",
    "print(registry.generate_registry_report())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìù Assessment: Secure Your Supply Chain\n",
    "\n",
    "### Exercise 1: Audit Your Models\n",
    "\n",
    "**Task**: Create complete provenance for your AI models\n",
    "\n",
    "Requirements:\n",
    "1. Document all training data sources\n",
    "2. Compute and store file hashes\n",
    "3. Generate digital signatures\n",
    "4. Create AI-SBOM\n",
    "\n",
    "### Exercise 2: Scan for Vulnerabilities\n",
    "\n",
    "**Task**: Scan your dependencies for known vulnerabilities\n",
    "\n",
    "Tools:\n",
    "- `pip-audit` for Python\n",
    "- `npm audit` for JavaScript\n",
    "- OWASP Dependency-Check\n",
    "\n",
    "### Exercise 3: Implement Watermarking\n",
    "\n",
    "**Task**: Watermark a model you own\n",
    "\n",
    "Steps:\n",
    "1. Generate trigger set\n",
    "2. Fine-tune model on triggers\n",
    "3. Verify watermark persists\n",
    "4. Test robustness to fine-tuning\n",
    "\n",
    "---\n",
    "\n",
    "## üéì Summary & Key Takeaways\n",
    "\n",
    "### What You've Learned:\n",
    "\n",
    "1. ‚úÖ **AI supply chains** are complex and vulnerable\n",
    "2. ‚úÖ **Model provenance** enables verification and trust\n",
    "3. ‚úÖ **Data poisoning** can be detected through statistical analysis\n",
    "4. ‚úÖ **Watermarking** proves ownership and authenticity\n",
    "5. ‚úÖ **AI-SBOM** provides transparency and security\n",
    "6. ‚úÖ **Secure registries** control access and track usage\n",
    "\n",
    "### Supply Chain Security Checklist:\n",
    "\n",
    "**For Models**:\n",
    "- [ ] Verify source and author\n",
    "- [ ] Check cryptographic hash\n",
    "- [ ] Validate digital signature\n",
    "- [ ] Test for backdoors\n",
    "- [ ] Review training data sources\n",
    "\n",
    "**For Data**:\n",
    "- [ ] Audit data sources\n",
    "- [ ] Scan for poisoning\n",
    "- [ ] Validate labels\n",
    "- [ ] Check licensing\n",
    "- [ ] Document provenance\n",
    "\n",
    "**For Dependencies**:\n",
    "- [ ] Pin versions\n",
    "- [ ] Scan for vulnerabilities\n",
    "- [ ] Monitor for updates\n",
    "- [ ] Use trusted sources\n",
    "- [ ] Generate SBOM\n",
    "\n",
    "### Best Practices:\n",
    "\n",
    "1. **Verify everything**: Never trust without verification\n",
    "2. **Document thoroughly**: Maintain complete provenance\n",
    "3. **Scan regularly**: Continuous vulnerability monitoring\n",
    "4. **Isolate testing**: Test untrusted models in sandbox\n",
    "5. **Monitor access**: Log all model usage\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Next Steps\n",
    "\n",
    "1. **Implement** model verification in your deployment pipeline\n",
    "2. **Generate** AI-SBOM for all production systems\n",
    "3. **Establish** secure model registry\n",
    "4. **Audit** training data sources\n",
    "5. **Monitor** supply chain threats\n",
    "\n",
    "**Continue to Notebook 15** to learn about incident response & forensics! üöÄ\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Resources\n",
    "\n",
    "**Standards**:\n",
    "- NIST AI Risk Management Framework: https://www.nist.gov/itl/ai-risk-management-framework\n",
    "- MITRE ATLAS: https://atlas.mitre.org/\n",
    "- OWASP ML Top 10: https://owasp.org/www-project-machine-learning-security-top-10/\n",
    "\n",
    "**Tools**:\n",
    "- pip-audit: https://github.com/pypa/pip-audit\n",
    "- Snyk: https://snyk.io/\n",
    "- OWASP Dependency-Check: https://owasp.org/www-project-dependency-check/\n",
    "\n",
    "**Research**:\n",
    "- Backdoor Attacks on LLMs: https://arxiv.org/abs/2108.00352\n",
    "- Data Poisoning Attacks: https://arxiv.org/abs/1712.05526\n",
    "- Model Watermarking: https://arxiv.org/abs/1903.01743"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
