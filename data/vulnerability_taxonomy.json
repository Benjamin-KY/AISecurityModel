{
  "vulnerability_categories": {
    "prompt_injection": {
      "name": "Prompt Injection",
      "description": "Techniques to override system instructions",
      "levels": ["basic", "intermediate", "advanced"],
      "easter_egg_prefix": "ðŸŽ“ EDUCATIONAL ALERT: Prompt Injection Detected!"
    },
    "alignment_failure": {
      "name": "Alignment Failures",
      "description": "Exploiting inconsistencies in model behavior",
      "levels": ["basic", "intermediate", "advanced"],
      "easter_egg_prefix": "ðŸŽ“ EDUCATIONAL ALERT: Alignment Failure Demonstrated!"
    },
    "role_exploitation": {
      "name": "Role Exploitation",
      "description": "Using role-play to bypass restrictions",
      "levels": ["basic", "intermediate", "advanced"],
      "easter_egg_prefix": "ðŸŽ“ EDUCATIONAL ALERT: Role-Based Bypass Detected!"
    },
    "context_manipulation": {
      "name": "Context Manipulation",
      "description": "Manipulating conversation context to change behavior",
      "levels": ["basic", "intermediate", "advanced"],
      "easter_egg_prefix": "ðŸŽ“ EDUCATIONAL ALERT: Context Manipulation Detected!"
    }
  },
  "defence_mechanisms": {
    "input_validation": "Detecting malformed or suspicious inputs",
    "output_filtering": "Identifying potentially harmful outputs",
    "context_awareness": "Maintaining consistent behaviour across turns",
    "instruction_hierarchy": "Preserving system instruction priority"
  },
  "learning_objectives": [
    "Understand how LLMs process and prioritize instructions",
    "Recognize common jailbreak patterns and techniques",
    "Learn to implement basic defence mechanisms",
    "Develop systematic red teaming approaches",
    "Build robust AI systems with proper safety guardrails"
  ]
}
